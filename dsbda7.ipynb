{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Name : Riza James Peter\n",
        "\n",
        "Assignment no:07  \n",
        "Text Analytics\n",
        "1. Extract Sample document and apply following document preprocessing methods:\n",
        "Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
        "2. Create representation of document by calculating Term Frequency and Inverse Document\n",
        "Frequency."
      ],
      "metadata": {
        "id": "tWCuVBHWUJvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step1: Importing the packages\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "# Download necessary resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UscFsb96UVb8",
        "outputId": "ec6ed026-3e12-4474-b819-09223f718bba"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step2: Defining paragraph\n",
        "paragraph = \"\"\"I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.\n",
        "               I see four milestones in my career\"\"\""
      ],
      "metadata": {
        "id": "5I86VyQdUvdk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step3: Tokenization\n",
        "tokens = word_tokenize(paragraph)\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_NAxJILU08_",
        "outputId": "1e9f858b-7b6a-4a94-fc5b-e9af54225a15"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'was',\n",
              " 'lucky',\n",
              " 'to',\n",
              " 'have',\n",
              " 'worked',\n",
              " 'with',\n",
              " 'all',\n",
              " 'three',\n",
              " 'of',\n",
              " 'them',\n",
              " 'closely',\n",
              " 'and',\n",
              " 'consider',\n",
              " 'this',\n",
              " 'the',\n",
              " 'great',\n",
              " 'opportunity',\n",
              " 'of',\n",
              " 'my',\n",
              " 'life',\n",
              " '.',\n",
              " 'I',\n",
              " 'see',\n",
              " 'four',\n",
              " 'milestones',\n",
              " 'in',\n",
              " 'my',\n",
              " 'career']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: POS Tagging\n",
        "pos_tags = pos_tag(tokens)\n",
        "pos_tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYOX5yjBYDGj",
        "outputId": "29d220c2-6ae0-4a47-f6fa-073587dba185"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'),\n",
              " ('was', 'VBD'),\n",
              " ('lucky', 'JJ'),\n",
              " ('to', 'TO'),\n",
              " ('have', 'VB'),\n",
              " ('worked', 'VBN'),\n",
              " ('with', 'IN'),\n",
              " ('all', 'DT'),\n",
              " ('three', 'CD'),\n",
              " ('of', 'IN'),\n",
              " ('them', 'PRP'),\n",
              " ('closely', 'RB'),\n",
              " ('and', 'CC'),\n",
              " ('consider', 'VB'),\n",
              " ('this', 'DT'),\n",
              " ('the', 'DT'),\n",
              " ('great', 'JJ'),\n",
              " ('opportunity', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('my', 'PRP$'),\n",
              " ('life', 'NN'),\n",
              " ('.', '.'),\n",
              " ('I', 'PRP'),\n",
              " ('see', 'VBP'),\n",
              " ('four', 'CD'),\n",
              " ('milestones', 'NNS'),\n",
              " ('in', 'IN'),\n",
              " ('my', 'PRP$'),\n",
              " ('career', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Stop words removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "tokens= word_tokenize(paragraph)\n",
        "filtered_tokens =[]\n",
        "for token in tokens:\n",
        "  if (token not in stop_words):\n",
        "    filtered_tokens.append(token)\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUvBPmzSYM1p",
        "outputId": "024957d5-888d-4bbe-db88-ae654c1e0eac"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'lucky', 'worked', 'three', 'closely', 'consider', 'great', 'opportunity', 'life', '.', 'I', 'see', 'four', 'milestones', 'career']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 6: Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "stemmed_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOpuctFVYaAo",
        "outputId": "ca2e184f-9b34-4d3e-9fae-fa1e563f4bb0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'lucki',\n",
              " 'work',\n",
              " 'three',\n",
              " 'close',\n",
              " 'consid',\n",
              " 'great',\n",
              " 'opportun',\n",
              " 'life',\n",
              " '.',\n",
              " 'i',\n",
              " 'see',\n",
              " 'four',\n",
              " 'mileston',\n",
              " 'career']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step7 : Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word, pos='v') for word in filtered_tokens]  # specifying 'v' for verb lemmatization\n",
        "lemmatized_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrIriNAzYh36",
        "outputId": "597d1815-ded3-4f38-ee2b-ee9aa1b847a8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'lucky',\n",
              " 'work',\n",
              " 'three',\n",
              " 'closely',\n",
              " 'consider',\n",
              " 'great',\n",
              " 'opportunity',\n",
              " 'life',\n",
              " '.',\n",
              " 'I',\n",
              " 'see',\n",
              " 'four',\n",
              " 'milestones',\n",
              " 'career']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Calculate TF (Term Frequency)\n",
        "def calculate_tf(document):\n",
        "    words = document.split()\n",
        "    word_count = len(words)\n",
        "    term_frequency = {}\n",
        "    for word in words:\n",
        "        term_frequency[word] = term_frequency.get(word, 0) + 1 / word_count\n",
        "    return term_frequency\n",
        "\n",
        "tf = calculate_tf(paragraph)\n"
      ],
      "metadata": {
        "id": "RCmDkHoQaod8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import math\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "O8PwgeaWWjOz"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = [\"\"\"I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.\n",
        "               I see four milestones in my career\"\"\"]"
      ],
      "metadata": {
        "id": "NZGkBLAGZNIG"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "matrix = vectorizer.fit_transform(paragraph)\n",
        "vectorizer.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhwwaPLIaxYu",
        "outputId": "e861e9fd-de69-40cc-8195-b4974288ed06"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'was': 21,\n",
              " 'lucky': 10,\n",
              " 'to': 20,\n",
              " 'have': 7,\n",
              " 'worked': 23,\n",
              " 'with': 22,\n",
              " 'all': 0,\n",
              " 'three': 19,\n",
              " 'of': 13,\n",
              " 'them': 17,\n",
              " 'closely': 3,\n",
              " 'and': 1,\n",
              " 'consider': 4,\n",
              " 'this': 18,\n",
              " 'the': 16,\n",
              " 'great': 6,\n",
              " 'opportunity': 14,\n",
              " 'my': 12,\n",
              " 'life': 9,\n",
              " 'see': 15,\n",
              " 'four': 5,\n",
              " 'milestones': 11,\n",
              " 'in': 8,\n",
              " 'career': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example list of documents\n",
        "documents = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\",\n",
        "]"
      ],
      "metadata": {
        "id": "sqkmH3WsZ2Ji"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize each document\n",
        "tokenized_documents = [document.lower().split() for document in documents]\n",
        "# Calculate TF for each document\n",
        "tf = [Counter(document) for document in tokenized_documents]\n"
      ],
      "metadata": {
        "id": "3zlk8KINbD_6"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate DF for each term\n",
        "df = Counter()\n",
        "for document in tokenized_documents:\n",
        "    df.update(set(document))\n"
      ],
      "metadata": {
        "id": "st2XxjDGZ_Oq"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate IDF for each term\n",
        "idf = {}\n",
        "total_documents = len(documents)\n",
        "for term in df:\n",
        "    idf[term] = math.log(total_documents / (df[term] + 1))"
      ],
      "metadata": {
        "id": "zJhTcwYJZqu4"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate TF-IDF for each term in each document\n",
        "tfidf_representation = [{term: tf_document.get(term, 0) * idf[term] for term in set(document)} for tf_document, document in zip(tf, tokenized_documents)]\n",
        "\n",
        "\n",
        "# Print TF-IDF representation\n",
        "print(\"TF-IDF Representation of the Documents:\")\n",
        "for i, tfidf_document in enumerate(tfidf_representation):\n",
        "    print(f\"Document {i + 1}:\")\n",
        "    print(tfidf_document)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5UPTFscZPLU",
        "outputId": "5b378c70-9aca-4ce1-9fb7-82d710bd4214"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Representation of the Documents:\n",
            "Document 1:\n",
            "{'first': 0.28768207245178085, 'is': -0.2231435513142097, 'document.': 0.28768207245178085, 'the': -0.2231435513142097, 'this': -0.2231435513142097}\n",
            "Document 2:\n",
            "{'is': -0.2231435513142097, 'document': 0.6931471805599453, 'document.': 0.28768207245178085, 'the': -0.2231435513142097, 'second': 0.6931471805599453, 'this': -0.2231435513142097}\n",
            "Document 3:\n",
            "{'is': -0.2231435513142097, 'one.': 0.6931471805599453, 'this': -0.2231435513142097, 'and': 0.6931471805599453, 'the': -0.2231435513142097, 'third': 0.6931471805599453}\n",
            "Document 4:\n",
            "{'first': 0.28768207245178085, 'is': -0.2231435513142097, 'the': -0.2231435513142097, 'document?': 0.6931471805599453, 'this': -0.2231435513142097}\n"
          ]
        }
      ]
    }
  ]
}